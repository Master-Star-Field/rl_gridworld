# RL Grid World Project

Проект для исследования и обучения RL-агентов (A2C, DRQN) в различных сеточных средах (GridWorld, MNIST). Для управления зависимостями и запуска используется **uv**.

## Установка

1. **Установите uv** (если не установлен):

   ```bash
   pip install uv
   ```

2. Синхронизируйте окружение:

Эта команда создаст виртуальное окружение и установит все зависимости (PyTorch, Gymnasium, WandB и др.).

   ```bash
   uv sync
   ```

3. Авторизация в WandB:
Проект активно использует логирование метрик в Weights & Biases.


   ```bash
   uv run wandb login
   ```

4. Автоматический запуск (Experiment Suite)

Для запуска серии экспериментов (GridSearch по сценариям, алгоритмам и типам сред) используйте скрипт auto_runs.py.

Этот скрипт автоматически перебирает сценарии сложности (s1...s4), алгоритмы (a2c, drqn) и настройки среды.

Команда запуска:

```bash
uv run python -m src.rl_grid_world.auto_runs
```


 * Инициализирует эксперименты согласно списку SCENARIOS.
* Запускает обучение (вызывая функции из train.py напрямую).
* Логирует прогресс в проект gridworld_experiment2 на WandB.
* Сохраняет GIF-анимации валидации и чекпоинты моделей.
* В конце выводит чек-лист со статусом выполнения каждого запуска.


5. Ручной запуск (Single Run)

Для запуска одиночного эксперимента с конкретными параметрами используйте модуль train.py.

Пример команды (DRQN на векторизованной среде):

```bash
uv run python -m src.rl_grid_world.train \
  --name "drqn_test" \
  --algo drqn \
  --env_type gym_vec \
  --h 10 --w 10 \
  --obstacle_ratio 0.1 \
  --max_steps 20000 \
  --see_obstacle
```

Основные аргументы CLI:

* --algo: Алгоритм обучения: a2c или drqn.
* --env_type: Тип среды:
        onehot: Классическая сетка (наблюдение = one-hot вектор позиции).
        mnist: Сетка, где клетки представлены изображениями цифр MNIST.
        gym_vec: Векторизованная среда на базе gymnasium.vector.
        np_vec: Оптимизированная Numpy-векторизованная среда.
* --h, --w: Размеры сетки (высота и ширина).
    --obstacle_ratio: Доля препятствий (от 0.0 до 1.0).
*  --see_obstacle: Флаг. Если указан, агент получает информацию о стенах в векторе наблюдения.
*  --episodes: Количество эпизодов обучения (только для A2C).
*  --max_steps: Количество шагов среды (только для DRQN).
*  --step_reward: Награда за каждый шаг (обычно отрицательная или 0).
*  --no-wandb: Отключить логирование в WandB.
